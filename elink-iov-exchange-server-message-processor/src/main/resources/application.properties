#---------------------rabbitmq配置start-----------------------------------------------

#rabbitmq配置
spring.rabbitmq.host=localhost
spring.rabbitmq.port=5672
spring.rabbitmq.username=admin
spring.rabbitmq.password=Admin@228
#spring.rabbitmq.virtualHost=test

#是否开启rabbitmq消息持久化特性，开启该特性会减低性能，可提高消息可靠性,平台各个服务配置请保持一致
rabbitmq.message.durable=true

#下行消息网关，平台发送消息时，把终端连接的网关id作为路由key，把消息发送到该消息网关即可
rabbitmq.downstream.message.exchange=elink.downstream.message.direct.exchange

#---------------------rabbitmq配置end-----------------------------------------------


##---------------------kafka配置start-----------------------------------------------
#
#spring.kafka.bootstrap-servers=cdh001:9092,cdh002:9092,cdh003:9092
## 发生错误后，消息重发的次数。
#spring.kafka.producer.retries=0
## 当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。
#spring.kafka.producer.batch-size=1638400
## 设置生产者内存缓冲区的大小。
#spring.kafka.producer.buffer-memory=33554432
## 键的序列化方式
#spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
## 值的序列化方式
#spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
## acks=0 ： 生产者在成功写入消息之前不会等待任何来自服务器的响应。
## acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应。
## acks=all ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。
#spring.kafka.producer.acks=1
#    #properties:
#      #max.request.size: 500000000
#      #request.timeout.ms: 60000
#spring.kafka.consumer.group-id=elink-iov-exchange-server-message-processor
## 自动提交的时间间隔 在spring boot 2.X 版本中这里采用的是值的类型为Duration 需要符合特定的格式，如1S,1M,2H,5D
#spring.kafka.consumer.auto-commit-interval=1S
## 该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：
## latest（默认值）在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的记录）
## earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录
#spring.kafka.consumer.auto-offset-reset=latest
## 是否自动提交偏移量，默认值是true,为了避免出现重复数据和数据丢失，可以把它设置为false,然后手动提交偏移量
#spring.kafka.consumer.enable-auto-commit=false
#
#两次poll()之间的最大间隔，默认值为5分钟。如果超过这个间隔同样会触发rebalance，文件处理耗费时间比较长，适当增加此值
#max.poll.interval.ms=36000000
##max.poll.records条数据需要在session.timeout.ms这个时间内处理完，默认：500
#spring.kafka.consumer.max-poll-records=50
##每次fetch请求时，server应该返回的最小字节数。如果没有足够的数据返回，请求会等待，直到足够的数据才会返回
##spring.kafka.consumer.fetch-min-size=1
##Fetch请求发给broker后，在broker中可能会被阻塞的（当topic中records的总size小于fetch.min.bytes时），此时这个fetch请求耗时就会比较长
##spring.kafka.consumer.fetch-max-wait=1S
#
## 键的反序列化方式
#spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
## 值的反序列化方式
#spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
## 在侦听器容器中运行的线程数。
#spring.kafka.listener.concurrency=1
# #listner负责ack，每调用一次，就立即commit
#spring.kafka.listener.ack-mode=manual_immediate
#spring.kafka.listener.missing-topics-fatal=false
#
##---------------------kafka配置end----------------------------------------------


#终端普通消息上行队列
elink.upstream.common.topic=elink-upstream-common-message

#终端紧急消息上行队列
elink.upstream.urgent.topic=elink-upstream-urgent-message

#终端位置消息上行队列
elink.upstream.gps.topic=elink-upstream-gps-message

#终端多媒体消息上行队列
elink.upstream.media.topic=elink-upstream-media-message

